{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#self.W = np.random.random((input_size, output_size))\n",
    "        #self.b =  np.random.random((output_size, 1))\n",
    "        self.W = np.random.randn(input_size, output_size) * np.sqrt(2.0 / (input_size + output_size)) \n",
    "        self.b = np.random.randn(output_size, 1) * np.sqrt(2.0 / (input_size + output_size)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Vanishing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotei os gradientes e percebi tamanho estupidamentes pequenos\n",
    "# [-2.03024384e-58]\n",
    "# [-4.06331200e-61]\n",
    "# [-1.24184324e-59]\n",
    "# [-2.24142734e-60]\n",
    "# [-1.19113176e-60]\n",
    "# [-4.63635878e-58]\n",
    "\n",
    "layers = [Layer(784, 64, ReLU()),\n",
    "          Layer(64, 32, ReLU()),\n",
    "          Layer(32, 2, ReLU()),\n",
    "          Layer(2, 32, ReLU()),\n",
    "          Layer(32, 64, ReLU()),\n",
    "          Layer(64, 784, Sigmoid())]\n",
    "\n",
    "nn = NeuralNetwork(layers)\n",
    "mse = MSE(nn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hadarmard product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example arrays\n",
    "array1 = np.array([[6, 2]])\n",
    "array2 = np.array([[5, 6]])\n",
    "\n",
    "# Hadamard product using the * operator\n",
    "result1 = array1 * array2\n",
    "\n",
    "# Hadamard product using numpy.multiply\n",
    "result2 = np.multiply(array1, array2)\n",
    "\n",
    "print(\"Using * operator:\")\n",
    "print(result1)\n",
    "\n",
    "print(\"\\nUsing numpy.multiply:\")\n",
    "print(result2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quando tem apenas uma sample entrando na rede:\n",
    "\n",
    "# l= 0\n",
    "# x shape (1, 1)\n",
    "# W shape (1, 4)\n",
    "# W.T @, x.shape ((4, 1), (1, 1)) = (4, 1) (TEMOS (4, UMA) ATIAVACOES)\n",
    "# B shape (4, 1)\n",
    "# a shape (4, 1)\n",
    "# z shape (4, 1)\n",
    "\n",
    "# l= 1\n",
    "# x shape (4, 1)\n",
    "# W shape (4, 5)\n",
    "# W.T @, x.shape ((5, 4), (4, 1)) = (5, 1)\n",
    "# B shape (5, 1)\n",
    "# a shape (5, 1)\n",
    "# z shape (5, 1)\n",
    "\n",
    "\n",
    "# quando tem apenas DUAS OU MAIS samples entrando na rede, nesse caso, um batch de tamanho quatro,\n",
    "# cada sample tem uma feature. \n",
    "# l= 0\n",
    "# x shape (1, 4)\n",
    "# W shape (1, 4)\n",
    "# W.T @, x.shape ((4, 1), (1, 4)) = (4, 4) (quatro ativacoes, nao (4, UMA))\n",
    "# B shape (4, 1)\n",
    "# a shape (4, 4)\n",
    "# z shape (4, 4)\n",
    "\n",
    "# l= 1\n",
    "# x shape (4, 4)\n",
    "# W shape (4, 5)\n",
    "# W.T @, x.shape ((5, 4), (4, 4)) = (5, 4)\n",
    "# B shape (5, 1)\n",
    "# a shape (5, 4)\n",
    "# z shape (5, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computo a media dos gradientes ao final do forward?\n",
    "# Nao. Sobre o gradiente global: uma matriz (batch) caminha no forward e volta uma matriz de gradientes no backward\n",
    "# multiplica a matriz de gradiente global pelo cache do vetor de ativacoes (da/dz) e dz/da = matriz de pesos  \n",
    "# E o gradiente mÃ©dio? \n",
    "# Agora sim. Quando eu for calcular o gradiente local (dz/dw) * global "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equivalencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_  = np.random.rand(1,784)\n",
    "out_ = np.random.rand(1,784)\n",
    "\n",
    "error = in_ - out_\n",
    "print(np.sum(error ** 2))\n",
    "print(error @ error.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3,4])\n",
    "b = np.array([5,6,7,8])\n",
    "\n",
    "diff = a - b\n",
    "print(np.sum(diff ** 2))\n",
    "diff @ diff.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(self):\n",
    "    if self.loss_type == \"MSE\":\n",
    "        error = self.output - self.input\n",
    "        squared_error = np.power( error, 2 )\n",
    "        sum_batch_error = np.sum(squared_error, axis=1, keepdims=True)\n",
    "        mean_batch_error = (1/self.m) * np.sum(sum_batch_error)\n",
    "        # 69334.9914932116\n",
    "        print(np.sum(error @ error.T)) \n",
    "        # 69334.9914932116\n",
    "        return mean_batch_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_batch = 8\n",
    "input = np.ones((8, 784)) + 1\n",
    "output = np.ones((8, 784))\n",
    "\n",
    "squared_error = np.power( input - output, 2 )\n",
    "print(squared_error.shape)\n",
    "\n",
    "input_tensor = torch.tensor(input, dtype=torch.float32)\n",
    "output_tensor = torch.tensor(output, dtype=torch.float32)\n",
    "\n",
    "mse = torch.nn.MSELoss()\n",
    "\n",
    "loss = mse(input_tensor, output_tensor)\n",
    "print(\"MSE Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = np.sum(squared_error, axis=1, keepdims=True)\n",
    "error.shape, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scalar\n",
    "np.sum(error) / m_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = 3\n",
    "# (3, 784)\n",
    "a = np.ones((3,784))\n",
    "np.sum(a, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae = AutoEncoder()\n",
    "\n",
    "# averiguar o pq esse valor tao alto\n",
    "# eu acho que eh normal, sao 784 val\n",
    "\n",
    "# testar a sigmoid \n",
    "\n",
    "np.sum(ae(single_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dadz_list = []\n",
    "dzda_prev_list = []\n",
    "\n",
    "layers = 8 - 1\n",
    "# nao estou considerando os pesos do [input] nem da primeira derivada\n",
    "# [dj/da] = ... [dz/dx]\n",
    "# pois aqui estou focado nas derivadas das camadas [ocultas]\n",
    "for l in range(layers, -1, -1):\n",
    "    print(ae_test.a_cache[l].shape)\n",
    "\n",
    "# (3, 784)\n",
    "# (3, 64)\n",
    "# (3, 32)\n",
    "# (3, 8)\n",
    "# (3, 2)\n",
    "# (3, 8)\n",
    "# (3, 32)\n",
    "# (3, 64)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assim vou percorrer do input ao output\n",
    "dadz_list = []\n",
    "dzda_prev_list = []\n",
    "for a, z, w in zip(ae_test.a_cache, ae_test.z_cache, ae_test.weights):\n",
    "    # dadz_list.append( z > 0 )\n",
    "    # dzda_prev_list.append( w )\n",
    "    print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assim, vou de 7 ate 0\n",
    "layers = 8 - 1\n",
    "for l in range(layers, -1, -1):\n",
    "    a = ae_test.a_cache[l]\n",
    "    z = ae_test.z_cache[l]\n",
    "    w = ae_test.weights[l]\n",
    "\n",
    "    dadz_list.append( z > 0 )\n",
    "    dzda_prev_list.append( w )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEI QUE ESTA NA ORDEM ERRADA, MAS EH SO PARA TER UMA IDEIA DAS DERIVADAS\n",
    "\n",
    "# derivatives (hidden layers) ie except\n",
    "# dj/da (the last diff) and\n",
    "# dz/dw (the firt diff)\n",
    "for a, z, w in zip(ae_test.a_cache, ae_test.z_cache, ae_test.weights):\n",
    "    # da/dz = d(max(0, z))/dz \n",
    "    # da/za = 1 (z > 0)\n",
    "    # dadz = 1 if z > 0 else 0\n",
    "    dadz = z > 0 \n",
    "    # dz/da_{l-1} = d(w @ a_{l-1} + b) / da_{l-1}\n",
    "    # dz/da_{l-1} = w\n",
    "    dzda_prev = w\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rascunho Backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1 sample\n",
    "x = np.array([[0.1], [0.5], [0.7]])\n",
    "batch = np.array( [[[0.1], [0.5], [0.7]],\n",
    "                    [[0.4], [0.6], [0.2]], \n",
    "                    [[0.9], [0.8], [0.3]],\n",
    "                    [[0.0], [1.0], [0.5]]])\n",
    "m = batch.shape[0]\n",
    "y = np.array([1, 2, 7, 3])\n",
    "\n",
    "# primeira camada\n",
    "np.array([[0.3], [0.4], [0.8]])\n",
    "np.array([0.5, 0.1, 0.1])\n",
    "\n",
    "\n",
    "W1 = np.array([[0.3, 0.4, 0.8], [0.5, 0.1, 0.1]])\n",
    "W2 = np.array([[0.6, 0.2]])\n",
    "\n",
    "B1 = np.ones((1,1))\n",
    "B2 = np.ones((1,1))\n",
    "\n",
    "biases = [B1, B2]\n",
    "\n",
    "weights = [W1, W2]\n",
    "\n",
    "\n",
    "\n",
    "def ReLU(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "\n",
    "\n",
    "z_cache = []\n",
    "a_cache = []\n",
    "\n",
    "def forward(x):\n",
    "    aux = x\n",
    "    for w, b in zip(weights, biases):\n",
    "        z = w @ aux + b\n",
    "        aux = ReLU(z)     \n",
    "\n",
    "        z_cache.append( z )\n",
    "        a_cache.append( aux )\n",
    "        \n",
    "    out = aux\n",
    "    return out \n",
    "\n",
    "def loss(pred, y):\n",
    "    m = pred.shape[0]\n",
    "    error = np.power( pred - y, 2 )\n",
    "    return (1/2*m) * np.sum( error )\n",
    "\n",
    "\n",
    "# forward \n",
    "# z1 a1 -> z2 a2 -> z3 a3 -> j\n",
    "# a3 = pred y_hat\n",
    "\n",
    "# j = 1/m * (a3 - y)Ë2\n",
    "\n",
    "# backward \n",
    "# z1 a1 <- z2 a2 <- z3 a3 <- j\n",
    "\n",
    "# df(g(x))/dx = df(g(x))/dg(x) * dg(x)/dx\n",
    "# pred = \n",
    "# j(a3(z3(a2(z2(a1(z1(wx)))))))\n",
    "\n",
    "# derivadas\n",
    "#     z1 a1     <-    z2 a2      <-      z3 a3     <-      j\n",
    "#    da1/dz1          da2/dz2           da3/dz3          dj/da3\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    " dj/dw1 = dj/da3 * da3/dz3 * dz3/da1 * da1/dz1 * dz1/dw1\n",
    " dj/dw2 = dj/da3 * da3/dz3 * dz3/da1 * da1/dz1 * dz1/dw2\n",
    " dj/dw3 = dj/da3 * da3/dz3 * dz3/da1 * da1/dz1 * dz1/dw3\n",
    "\n",
    "\n",
    " dj/da3\n",
    "    dj/da3 = 1/m * np.sum( a3 - y ) * 1 \n",
    "    dj/da3 = 1/m * np.sum( a3 - y ) * dj/a3 (a3 - y)\n",
    "    dj/da3 = 1/m * np.sum( a3 - y ) * da3/a3 - dy/da3\n",
    "    dj/da3 = 1/m * np.sum( a3 - y ) * 1 - 0 \n",
    "    dj/da3 = 1/m * np.sum( a3 - y ) \n",
    "\n",
    " da3/dz3 \n",
    "    da3/dz3 = dmax(0, z3)/dz3 \n",
    "    da3/dz3 = 1(z3 > 0)\n",
    "\n",
    " dz3/da1 \n",
    "    dz3/da1 = d(a1*w7 + a2*w8)/da1\n",
    "    dz3/da1 = w7\n",
    "\n",
    " da1/dz1 \n",
    "    da1/dz1 = d(max(0, z1))/dz1\n",
    "    da1/dz1 = 1(z1 > 0)\n",
    "\n",
    " dz1/dw1 \n",
    "    dz1/dw1 = d(x1w1 + x2w2 + x3w3)/dw1\n",
    "    dz1/dw1 = x1\n",
    "\n",
    " dz1/dw2 \n",
    "    dz1/dw2 = d(x1w1 + x2w2 + x3w3)/dw2\n",
    "    dz1/dw2 = x2\n",
    "\n",
    " dz1/dw3 \n",
    "    dz1/dw3 = d(x1w1 + x2w2 + x3w3)/dw3\n",
    "    dz1/dw3 = x3\n",
    "\n",
    "   \n",
    "          comum entre \n",
    "          w1,w2,...,w6\n",
    " dj/dw4 = dj/da3 * da3/dz3 *                dz3/da2 * da2/dz2 * dz2/dw4\n",
    " dj/dw5 = dj/da3 * da3/dz3 *                dz3/da2 * da2/dz2 * dz2/dw5 \n",
    " dj/dw6 = dj/da3 * da3/dz3 *                dz3/da2 * da2/dz2 * dz2/dw6\n",
    "\n",
    " (1/m * np.sum( a3 - y )) * 1(z3 > 0) * \n",
    "\n",
    "\n",
    " dz3/da2 \n",
    "      dz3/da2 = d(a1*w7 + a2*w8)/da2\n",
    "      dz3/da2 = w8\n",
    "   \n",
    "\n",
    "da2/dz2\n",
    "      da2/dz2 = d(max(0, z2)) = 1(z2 > 0)\n",
    "\n",
    "\n",
    "# segunda camada\n",
    "\n",
    "dj/dw7 = dj/da3 * da3/dz3 * dz3/dw7\n",
    "dj/dw8 = dj/da3 * da3/dz3 * dz3/dw8\n",
    "\n",
    "dz3/dw7 \n",
    "\n",
    "      d(a1*w7 + a2*w8)/dw7 = a1\n",
    "      \n",
    "dz3/dw8\n",
    "      d(a1*w7 + a2*w8)/dw8 = a2\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# dj/dw1,w2,w3 = (1/m * np.sum(a3 - y)) * (1 * (z3 > 0)) * w7 * (1 * (z1 > 0)) * x1 * x2 * x3  \n",
    "# gradiente 1 \n",
    "# g1 = (1/m * np.sum(a3 - y)) * (1 * (z3 > 0)) * w7 * (1 * (z1 > 0)) * x1 * x2 * x3  \n",
    "\n",
    "# dz/dw = [ [x1, x2, x3],\n",
    "#           [x1, x2, x3] ]\n",
    "\n",
    "pred = forward(x) \n",
    "\n",
    "\"\"\" \n",
    "print(z_cache)\n",
    "[\n",
    "    ([[1.79], [1.17]]), \n",
    "    ([[2.308]])\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# for z, a in zip(z_cache, a_cache):\n",
    "\n",
    "n_layers = 2\n",
    "m = batch.shape[0]\n",
    "# 1.79 1.52 1.83 1.8 sao os z1\n",
    "\"\"\"\n",
    "   l in 0, 1\n",
    "   z_cache[l].T\n",
    "   [[[1.79 1.52 1.83 1.8 ] z1  layer 0\n",
    "   [1.17 1.28 1.56 1.15]]] z2  layer 0 \n",
    "   [[[2.308 2.168 2.41  2.31 ]]] z3 layer 1\n",
    "\n",
    "print( z_cache[0].T[0][0] ) z1 of layer 1 >> [ 1.79 1.52 1.83 1.8 ] \n",
    "print( z_cache[1].T[0][0] ) z3 of layer 2 >> [ 2.308 2.168 2.41  2.31 ]\n",
    "\n",
    "print( a_cache[0].T[0][0] ) a1 of layer 1 >> [ 1.79 1.52 1.83 1.8 ] \n",
    "print( a_cache[1].T[0][0] ) a3 of layer 2 >> [ 2.308 2.168 2.41  2.31 ]\n",
    "\n",
    "\n",
    "z1_batch = z_cache[0].T[0][0] >>> [ 1.79 1.52 1.83 1.8 ] \n",
    "z2_batch = z_cache[0].T[0][1] >>> [1.17, 1.28, 1.56, 1.15]\n",
    "z3_batch = z_cache[1].T[0][0] >>> [ 2.308 2.168 2.41  2.31 ]\n",
    "\n",
    "a1_batch = a_cache[0].T[0][0] >>> [1.79 1.52 1.83 1.8 ]\n",
    "a2_batch = a_cache[0].T[0][1] >>> [1.17 1.28 1.56 1.15]\n",
    "a3_batch = a_cache[1].T[0][0] >>> [2.308 2.168 2.41  2.31 ]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def get_z_and_a_batch(z_cache, a_cache):\n",
    "   z1 = z_cache[0].T[0][0] \n",
    "   z2 = z_cache[0].T[0][1] \n",
    "   z3 = z_cache[1].T[0][0]  \n",
    "\n",
    "   a1 = a_cache[0].T[0][0]\n",
    "   a2 = a_cache[0].T[0][1]\n",
    "   a3 = a_cache[1].T[0][0]\n",
    "\n",
    "   return z1, z2, z3, a1, a2, a3\n",
    "\n",
    "\"\"\"\n",
    "batch.T\n",
    "\n",
    "         \n",
    "array([[[0.1, 0.4, 0.9, 0. ],     x1 de cada sample\n",
    "        [0.5, 0.6, 0.8, 1. ],     x2 de cada sample\n",
    "        [0.7, 0.2, 0.3, 0.5]]])   x3 de cada sample\n",
    "\"\"\"\n",
    "\n",
    "def get_features_batch(batch):\n",
    "   x1 = batch.T[0][0]\n",
    "   x2 = batch.T[0][1]\n",
    "   x3 = batch.T[0][2]\n",
    "   \n",
    "   return np.array([x1, x2, x3])\n",
    "\n",
    "def get_w78(weights):\n",
    "   return np.array(weights[1][0])\n",
    "\n",
    "\n",
    "z1, z2, z3, a1, a2, a3 = get_z_and_a_batch(z_cache, a_cache)\n",
    "\n",
    "x1x2x3 = get_features_batch(batch)\n",
    "w7, w8 = get_w78(weights)\n",
    "a1a2 = np.array([a1, a2])\n",
    "\n",
    "same_to_all = (1/m * np.sum( a3 - y )) * (z3 > 0)\n",
    "dj_dw123 = [same_to_all * w7 * (z1 > 0)] * x1x2x3\n",
    "dj_dw456 = [same_to_all * w8 * (z2 > 0)] * x1x2x3\n",
    "dj_dw78 =  [same_to_all * a1a2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = (.3*.1 + .4*.5 + .8*.7) + np.ones((1,1))\n",
    "z2 = (.5*.1 + .1*.5 + .1*.7) + np.ones((1,1))\n",
    "z3 = z1*.6 + z2*.2 + np.ones((1,1))\n",
    "\n",
    "print(f\"z1 {z1}\")\n",
    "print(f\"z2 {z2}\")\n",
    "print(f\"z {z2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRINT GRADS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Definir a arquitetura do autoencoder\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(784, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 784),\n",
    "            nn.Sigmoid()  # Usamos Sigmoid para a camada de saÃ­da para limitar os valores entre 0 e 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Criar uma instÃ¢ncia do autoencoder\n",
    "autoencoder = Autoencoder()\n",
    "\n",
    "# Definir um valor aleatÃ³rio como entrada (batch size = 1, dimensÃ£o = 784)\n",
    "input_data = torch.rand(1, 784)\n",
    "\n",
    "# Calcular os gradientes\n",
    "output = autoencoder(input_data)\n",
    "output.mean().backward()\n",
    "\n",
    "# Imprimir o shape dos gradientes para cada parÃ¢metro\n",
    "for name, param in autoencoder.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        if \"bias\" not in name:\n",
    "            print(f\"Nome: {name}, Shape do Gradiente: {param.grad.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tudo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb6d6c4719b3a18b49a5a3f10581ccd97d0424968e27468a36ffe3dc55659e32"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
