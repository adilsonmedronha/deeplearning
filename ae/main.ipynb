{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO testar a rede com XOR\n",
    "\n",
    "# TODO revisar o backprop, tentar deixar tudo igual as minhas anotacoes,\n",
    "# questoes de transposicao etc, se nao der, pq n deu?\n",
    "\n",
    "# TODO fiz classe abstrata Loss, criar um .py para cada loss especializada?\n",
    "# tbm uma classe para cada otimizador? \n",
    "# como organizar melhor as classes e modulos do projeto?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.34806238]] oi\n",
      "NeuralNetwork(\n",
      "   Layer 0: (1, 1) \n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nn import NeuralNetwork\n",
    "from loss import MSE\n",
    "from activation import ReLU, Sigmoid\n",
    "from layer import Layer\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "\n",
    "layers = [Layer(1, 4, \n",
    "          ReLU()), \n",
    "          Layer(4, 5, \n",
    "          ReLU()),\n",
    "          Layer(5, 6, \n",
    "          ReLU()),\n",
    "          Layer(6, 7, \n",
    "          ReLU()),\n",
    "          Layer(7, 2, \n",
    "          ReLU())]\n",
    "\n",
    "\n",
    "layers = [Layer(1, 1, \n",
    "          Sigmoid())]\n",
    "\n",
    "\n",
    "# layers = [Layer(784, 32, \n",
    "#           ReLU()), \n",
    "#           Layer(32, 784, \n",
    "#           Sigmoid())]\n",
    "\n",
    "x = np.random.random((1, 1))\n",
    "nn_my = NeuralNetwork(layers)\n",
    "mse = MSE(nn_my)\n",
    "l2 = mse(x, x)\n",
    "pred = nn_my(x)\n",
    "print(nn_my)\n",
    "mse.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"data/Housing.csv\")\n",
    "X_house, Y_house = data.price, data.area\n",
    "X_house = np.array(X_house)\n",
    "Y_house = np.array(Y_house)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class NNT(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NNT, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(784, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 784),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        activations = []\n",
    "        for layer in self.model:\n",
    "            x = layer(x)\n",
    "            activations.append(x.clone())\n",
    "        return x, activations\n",
    "\n",
    "    def forward_original(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "from torch import optim\n",
    "nnt = NNT()\n",
    "opt = optim.SGD(nnt.parameters(), lr=0.05)\n",
    "loss_t = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = MNIST(root='data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xor_data = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "xor_y = np.array([[0, 1, 1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer(nn):\n",
    "    for layer in nn.layers:\n",
    "        layer.W -= 0.0005 * layer.grad_local.T\n",
    "        layer.b -= 0.0005 * layer.delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_grad(nn):\n",
    "    for layer in nn.layers:\n",
    "        layer.grad_local = 0\n",
    "        layer.delta = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "for layer in nn_my.layers:\n",
    "    print(np.sum(layer.grad_local))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Optimizer(ABC):\n",
    "    def __init__(self, model: NeuralNetwork, lr: float):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "\n",
    "    @abstractmethod\n",
    "    def step(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def zero_grad(self):\n",
    "        pass\n",
    "\n",
    "class Adam(Optimizer):\n",
    "    \n",
    "    def step(self):\n",
    "        layers = self.model.get_layers()\n",
    "        for layer in layers:\n",
    "            # TODO investigar esses transpostos, tentar deixar o backprop igual as minhas anotacoes?\n",
    "            print( f\"grad local {layer.grad_local.T}\")\n",
    "            print( f\" layer delta {layer.delta}\")\n",
    "            layer.W -= self.lr * layer.grad_local.T\n",
    "            layer.b -= self.lr * layer.delta\n",
    "\n",
    "    def zero_grad(self):\n",
    "        layers = self.model.get_layers()\n",
    "        for layer in layers:\n",
    "            layer.delta = 0\n",
    "            layer.grad_local = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(nn_my, lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-23441782.02101636]] oi\n",
      "[[0.]] oi\n",
      "x [13300000]\n",
      "w [[-1.76254006]]\n",
      "b [[0.73817561]]\n",
      "pred [[0.]]\n",
      "loss 176890000000000.0\n",
      "grad local [0.]\n",
      " layer delta [[-0.]]\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "LOSS_T = []\n",
    "LOSS = []\n",
    "import torch\n",
    "for i in range(EPOCHS):\n",
    "    for x, y in zip(X_house, Y_house):\n",
    "        x = np.array([x])\n",
    "        y = np.array([y])\n",
    "        pred = nn_my(x)\n",
    "        loss = mse(pred, x)\n",
    "        print(\"x\", x)\n",
    "        print(\"w\", nn_my.layers[0].W)\n",
    "        print(\"b\", nn_my.layers[0].b)\n",
    "        print(\"pred\", pred)\n",
    "        print(\"loss\" , loss)\n",
    "        LOSS.append(loss)\n",
    "        mse.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176890000000000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "13300000 ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance( np.random.randn(1, 1)[0][0], np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-83692974700.0\n"
     ]
    }
   ],
   "source": [
    "print(-8.36929747e+10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/adilson/Git/deeplearning/ae/main.ipynb Cell 14\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/adilson/Git/deeplearning/ae/main.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/adilson/Git/deeplearning/ae/main.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/adilson/Git/deeplearning/ae/main.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m#for data in train_loader:\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/adilson/Git/deeplearning/ae/main.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m#x, _ = data\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/adilson/Git/deeplearning/ae/main.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m#x = x.reshape((784, -1)).numpy()\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/adilson/Git/deeplearning/ae/main.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     pred \u001b[39m=\u001b[39m nn_my(a)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/adilson/Git/deeplearning/ae/main.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     loss \u001b[39m=\u001b[39m mse(pred, a)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/adilson/Git/deeplearning/ae/main.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     LOSS\u001b[39m.\u001b[39mappend(loss)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10000\n",
    "LOSS_T = []\n",
    "LOSS = []\n",
    "import torch\n",
    "for i in range(EPOCHS):\n",
    "    #for data in train_loader:\n",
    "    #x, _ = data\n",
    "    #x = x.reshape((784, -1)).numpy()\n",
    "    pred = nn_my(a)\n",
    "    loss = mse(pred, a)\n",
    "    LOSS.append(loss)\n",
    "    mse.backward()\n",
    "    optimizer(nn_my)\n",
    "    #zero_grad(nn_my)\n",
    "    ## torch\n",
    "    #x = x.reshape((-1, 784))\n",
    "    #x = torch.asarray(x)\n",
    "    #pred_t, activations = nnt(x)\n",
    "    #loss = loss_t(pred_t, x)\n",
    "    #LOSS_T.append(loss)\n",
    "    #loss.backward()\n",
    "    #opt.step()\n",
    "    #opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5-1, -1, -1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "input_image_tensor = a.reshape(1, 1, 28, 28)   # torch.Tensor\n",
    "torch_image_tensor = pred_t.detach().reshape(1, 1, 28, 28)  # torch.Tensor\n",
    "mine_image_tensor = pred.reshape(1, 1, 28, 28) # numpy.ndarray\n",
    "\n",
    "\n",
    "input_image_np = input_image_tensor.squeeze()  # Remove singleton dimensions\n",
    "torch_image_np = torch_image_tensor.squeeze().numpy()\n",
    "mine_image_np = mine_image_tensor.squeeze()\n",
    "diff = input_image_np - torch_image_np\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(12, 4))\n",
    "\n",
    "axes[0].imshow(input_image_np, cmap='gray')  # Use 'gray' colormap for grayscale images\n",
    "axes[0].set_title('Input Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(torch_image_np, cmap='gray')\n",
    "axes[1].set_title('Torch Image')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(mine_image_np, cmap='gray')\n",
    "axes[2].set_title('Mine Image')\n",
    "axes[2].axis('off')\n",
    "\n",
    "axes[3].imshow(diff, cmap='gray')\n",
    "axes[3].set_title('Difference')\n",
    "axes[3].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "random_array = np.random.uniform(low=-1, high=1, size=(3, 4))\n",
    "print(random_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_array = np.random.randn(3, 3) * np.sqrt(2.0 / (3 + 3))  \n",
    "numpy_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.Linear(3,3).weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.Linear(3,3).bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(LOSS, label='Loss')\n",
    "plt.xlabel('Batch Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Over Batch Iterations')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l, layer in enumerate(nn_my.layers):\n",
    "    print(f'l= {l}')\n",
    "    print(f'x shape {layer.x.shape}')\n",
    "    print(f'W shape {layer.W.shape}')\n",
    "    print(f'W.T @, x.shape {(layer.W.T.shape, layer.x.shape)} = {(layer.W.T @ layer.x).shape}')\n",
    "    print(f'B shape {layer.b.shape}')\n",
    "    print(f'a shape {layer.a.shape}')\n",
    "    print(f'z shape {layer.z.shape}')\n",
    "    print(f'GRADS')\n",
    "    print(f'local {layer.grad_local.shape}')\n",
    "    print(f'delta {layer.delta.shape}\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tudo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
